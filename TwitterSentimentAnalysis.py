"""Sentiment analysis of twitter dataset.ipynb
Automatically generated by Colaboratory.
# Basic interpretation of data by machine learning model
*   Input is given as string to the computer that is converted into machine     understandable language
*   The process of tokenization is used in tensorflow to convert the string into numbers. 
*   Strings -> tokens -> integers -> vectors
*   The process of taking out the tokens out of the sentences is tokenizing. Tokens basically mean the individual words in the sentence. example:
sentence: ['i love my cat']
tokens: ['i', 'love', 'my', 'cat']
*   The tokenizer then converts these tokens into sequence of integers where each word is given its word index.  example: 
tokens: [['i', 'love', 'my', 'cat'][ 'i', 'have', 'a', 'cat']]
sequence: [[1, 2, 3, 4][1, 5, 6, 4]]
*   Padding is done for each sequence so that the sequences are of same length
# Shape of model
*   Keras functional API is used for building model
*   `i = Input(shape=(T,))                    # T is the length of sequence `            
*   `x = Embedding(V, D)(i)               # V = Vocab Size, D = Embedding dimensionality   `    
*   `x = LSTM(M)(x)                   # M = Hidden vector dimensionality`
*   `x = GlobalMaxPooling1D()(x)`
*   `x = Dense(K, activation='sigmoid')    # K = no. of output classes`
# Workflow of the Model
The following are the steps required to build the mode:
1.   Importing the required modules
2.   Loading the data
3.   Preprocessing the data
4.   Building the model
5.   Evaluating the model
6.   Prediction
"""

# mounting the drive 
from google.colab import drive
drive.mount('/content/drive/')

# Importing libraries and modules
import tensorflow as tf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# reading the data in a dataframe df
df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/Sentiment Analysis Dataset.csv', encoding='latin-1')

# printing the dataframe 
df.head(10)

# drop the unnecessary columns
df.drop('ItemID', axis=1, inplace=True)
df.head(10)

# veryfying the sentiment values
# 1 is positive sentiment and 0 is negative sentiment
df['Sentiment'].value_counts()

# pre-processing the data
# define a function to remove the @mentions and other useless text from the tweets

import re

def text_cleaning(text):
  text = re.sub(r'@[A-Za-z0-9]+', '', text)     # removing @mentions
  text = re.sub(r'@[A-Za-zA-Z0-9]+', '', text)  # removing @mentions 
  text = re.sub(r'@[A-Za-z]+', '', text)        # removing @mentions
  text = re.sub(r'@[-)]+', '', text)            # removing @mentions
  text = re.sub(r'#', '', text )                # removing '#' sign
  text = re.sub(r'RT[\s]+', '', text)           # removing RT
  text = re.sub(r'https?\/\/\S+', '', text)     # removing the hyper link
  text = re.sub(r'&[a-z;]+', '', text)          # removing '&gt;'

  return text

# applying the text cleaning function on tweets
df['SentimentText'] = df['SentimentText'].apply(text_cleaning)
df.head(10)

# splitting the data into training and testing data

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(df['SentimentText'].values, df['Sentiment'].values, test_size=0.30)

# checking the data split
print('sentiment Text: ', x_train[0])
print('sentiment: ', y_train[0])

# converting the strings into integers using Tokenizer 
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences 

# instantiating the tokenizer
max_vocab = 20000000
tokenizer = Tokenizer(num_words=max_vocab)
tokenizer.fit_on_texts(x_train)

# checking the word index and find out the vocabulary of the dataset
wordidx = tokenizer.word_index
V = len(wordidx)
print('The size of datatset vocab is: ', V)

# converting tran and test sentences into sequences
train_seq = tokenizer.texts_to_sequences(x_train)
test_seq = tokenizer.texts_to_sequences(x_test)
print('Training sequence: ', train_seq[0])
print('Testing sequence: ', test_seq[0])

# padding the sequences to get equal length sequence because its conventional to use same size sequences
# padding the traing sequence
pad_train = pad_sequences(train_seq)
T = pad_train.shape[1]
print('The length of training sequence is: ', T)

# padding the test sequence
pad_test = pad_sequences(test_seq, maxlen=T)
print('The length of testing sequence is: ', pad_test.shape[1])

# building the model

from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, GlobalMaxPooling1D
from tensorflow.keras.models import Model

D = 20 
M = 15

i = Input (shape=(T, ))   
x = Embedding(V+1, D)(i)    # V+1 because the indexing of the words in vocab (V) start from 1 not 0
x = LSTM(M, return_sequences=True)(x)
x = GlobalMaxPooling1D()(x)
x = Dense(32, activation='relu')(x)
x = Dense(1, activation='sigmoid')(x)

model = Model(i,x)

# compiling the model
model.compile(optimizer='adam', 
              loss='binary_crossentropy', 
              metrics=['accuracy'])

# training the model
r = model.fit(pad_train, y_train, validation_data=(pad_test, y_test), epochs=2)

# Evaluating the model
# plotting the loss and validation loss of the model
plt.plot(r.history['loss'], label='loss')
plt.plot(r.history['val_loss'], label = 'val_loss')
plt.legend()

# plotting the accuracy and validation accuracy of the model
plt.plot(r.history['accuracy'], label= 'accuracy')
plt.plot(r.history['val_accuracy'], label='val_accuracy')
plt.legend()

# Predicting the sentiment of any text

def predict_sentiment(text):
  # preprocessing the given text 
  text_seq = tokenizer.texts_to_sequences(text)
  text_pad = pad_sequences(text_seq, maxlen=T)

  # predicting the class
  predicted_sentiment = model.predict(text_pad).round()

  if predicted_sentiment == 1.0:
    return(print('It is a positive sentiment'))
  else:
    return(print('It is a negative sentiment'))

text = ['i feel happy']
predict_sentiment(text)

# saving the model for future purpose
model.save('sentiment analysis.h5')   # creates HDF5 file for model
